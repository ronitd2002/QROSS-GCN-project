{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronitd2002/QROSS-GCN-project/blob/main/trial-notebooks/QROSS_Naive-Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vhDYBPid46wU"
      },
      "outputs": [],
      "source": [
        "# basic libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn9RExX13YEA"
      },
      "source": [
        "### Importing and normalizing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "i3U7z0a4YA3I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9VwhAhThfa_8",
        "outputId": "a9c22b8a-d592-4aca-c21c-c35ec9f60d31"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3495f6eb-001e-4b49-bfba-ccddeae523c5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3495f6eb-001e-4b49-bfba-ccddeae523c5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving syn_tsp199.tsp to syn_tsp199.tsp\n",
            "Saving syn_tsp198.tsp to syn_tsp198.tsp\n",
            "Saving syn_tsp197.tsp to syn_tsp197.tsp\n",
            "Saving syn_tsp196.tsp to syn_tsp196.tsp\n",
            "Saving syn_tsp195.tsp to syn_tsp195.tsp\n",
            "Saving syn_tsp194.tsp to syn_tsp194.tsp\n",
            "Saving syn_tsp193.tsp to syn_tsp193.tsp\n",
            "Saving syn_tsp192.tsp to syn_tsp192.tsp\n",
            "Saving syn_tsp191.tsp to syn_tsp191.tsp\n",
            "Saving syn_tsp190.tsp to syn_tsp190.tsp\n",
            "Saving syn_tsp189.tsp to syn_tsp189.tsp\n",
            "Saving syn_tsp188.tsp to syn_tsp188.tsp\n",
            "Saving syn_tsp187.tsp to syn_tsp187.tsp\n",
            "Saving syn_tsp186.tsp to syn_tsp186.tsp\n",
            "Saving syn_tsp185.tsp to syn_tsp185.tsp\n",
            "Saving syn_tsp139.tsp to syn_tsp139.tsp\n",
            "Saving syn_tsp138.tsp to syn_tsp138.tsp\n",
            "Saving syn_tsp137.tsp to syn_tsp137.tsp\n",
            "Saving syn_tsp136.tsp to syn_tsp136.tsp\n",
            "Saving syn_tsp135.tsp to syn_tsp135.tsp\n",
            "Saving syn_tsp134.tsp to syn_tsp134.tsp\n",
            "Saving syn_tsp133.tsp to syn_tsp133.tsp\n",
            "Saving syn_tsp132.tsp to syn_tsp132.tsp\n",
            "Saving syn_tsp131.tsp to syn_tsp131.tsp\n",
            "Saving syn_tsp130.tsp to syn_tsp130.tsp\n",
            "Saving syn_tsp129.tsp to syn_tsp129.tsp\n",
            "Saving syn_tsp128.tsp to syn_tsp128.tsp\n",
            "Saving syn_tsp127.tsp to syn_tsp127.tsp\n",
            "Saving syn_tsp126.tsp to syn_tsp126.tsp\n",
            "Saving syn_tsp125.tsp to syn_tsp125.tsp\n",
            "Saving syn_tsp124.tsp to syn_tsp124.tsp\n",
            "Saving syn_tsp123.tsp to syn_tsp123.tsp\n",
            "Saving syn_tsp122.tsp to syn_tsp122.tsp\n",
            "Saving syn_tsp121.tsp to syn_tsp121.tsp\n",
            "Saving syn_tsp120.tsp to syn_tsp120.tsp\n",
            "Saving syn_tsp119.tsp to syn_tsp119.tsp\n",
            "Saving syn_tsp118.tsp to syn_tsp118.tsp\n",
            "Saving syn_tsp117.tsp to syn_tsp117.tsp\n",
            "Saving syn_tsp116.tsp to syn_tsp116.tsp\n",
            "Saving syn_tsp115.tsp to syn_tsp115.tsp\n",
            "Saving syn_tsp114.tsp to syn_tsp114.tsp\n",
            "Saving syn_tsp113.tsp to syn_tsp113.tsp\n",
            "Saving syn_tsp112.tsp to syn_tsp112.tsp\n",
            "Saving syn_tsp111.tsp to syn_tsp111.tsp\n",
            "Saving syn_tsp110.tsp to syn_tsp110.tsp\n",
            "Saving syn_tsp109.tsp to syn_tsp109.tsp\n",
            "Saving syn_tsp108.tsp to syn_tsp108.tsp\n",
            "Saving syn_tsp107.tsp to syn_tsp107.tsp\n",
            "Saving syn_tsp106.tsp to syn_tsp106.tsp\n",
            "Saving syn_tsp105.tsp to syn_tsp105.tsp\n",
            "Saving syn_tsp104.tsp to syn_tsp104.tsp\n",
            "Saving syn_tsp103.tsp to syn_tsp103.tsp\n",
            "Saving syn_tsp102.tsp to syn_tsp102.tsp\n",
            "Saving syn_tsp101.tsp to syn_tsp101.tsp\n",
            "Saving syn_tsp99.tsp to syn_tsp99.tsp\n",
            "Saving syn_tsp98.tsp to syn_tsp98.tsp\n",
            "Saving syn_tsp97.tsp to syn_tsp97.tsp\n",
            "Saving syn_tsp96.tsp to syn_tsp96.tsp\n",
            "Saving syn_tsp95.tsp to syn_tsp95.tsp\n",
            "Saving syn_tsp94.tsp to syn_tsp94.tsp\n",
            "Saving syn_tsp93.tsp to syn_tsp93.tsp\n",
            "Saving syn_tsp92.tsp to syn_tsp92.tsp\n",
            "Saving syn_tsp91.tsp to syn_tsp91.tsp\n",
            "Saving syn_tsp90.tsp to syn_tsp90.tsp\n",
            "Saving syn_tsp89.tsp to syn_tsp89.tsp\n",
            "Saving syn_tsp88.tsp to syn_tsp88.tsp\n",
            "Saving syn_tsp87.tsp to syn_tsp87.tsp\n",
            "Saving syn_tsp86.tsp to syn_tsp86.tsp\n",
            "Saving syn_tsp85.tsp to syn_tsp85.tsp\n",
            "Saving syn_tsp84.tsp to syn_tsp84.tsp\n",
            "Saving syn_tsp83.tsp to syn_tsp83.tsp\n",
            "Saving syn_tsp82.tsp to syn_tsp82.tsp\n",
            "Saving syn_tsp81.tsp to syn_tsp81.tsp\n",
            "Saving syn_tsp79.tsp to syn_tsp79.tsp\n",
            "Saving syn_tsp78.tsp to syn_tsp78.tsp\n",
            "Saving syn_tsp77.tsp to syn_tsp77.tsp\n",
            "Saving syn_tsp76.tsp to syn_tsp76.tsp\n",
            "Saving syn_tsp75.tsp to syn_tsp75.tsp\n",
            "Saving syn_tsp74.tsp to syn_tsp74.tsp\n",
            "Saving syn_tsp73.tsp to syn_tsp73.tsp\n",
            "Saving syn_tsp72.tsp to syn_tsp72.tsp\n",
            "Saving syn_tsp71.tsp to syn_tsp71.tsp\n",
            "Saving syn_tsp70.tsp to syn_tsp70.tsp\n",
            "Saving syn_tsp69.tsp to syn_tsp69.tsp\n",
            "Saving syn_tsp68.tsp to syn_tsp68.tsp\n"
          ]
        }
      ],
      "source": [
        "# upload training data\n",
        "from google.colab import files\n",
        "\n",
        "uploaded_1 = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "OPMzQgb9fKPn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "graphs = {}\n",
        "index = 0\n",
        "\n",
        "for filename, file_data in uploaded_1.items():\n",
        "    file_content = file_data.decode('utf-8')\n",
        "\n",
        "    start_collecting = False\n",
        "    coordinates = []\n",
        "\n",
        "    for line in file_content.splitlines():\n",
        "        if \"NODE_COORD_SECTION\" in line:\n",
        "            start_collecting = True\n",
        "            continue\n",
        "\n",
        "        if \"EOF\" in line or line.strip() == '':\n",
        "            break\n",
        "\n",
        "        if start_collecting:\n",
        "            parts = line.split()\n",
        "            if len(parts) >= 3:\n",
        "                # Append [x, y] coordinates (ignore the first column)\n",
        "                coordinates.append([float(parts[1]), float(parts[2])])\n",
        "\n",
        "    graphs[index] = coordinates\n",
        "    index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRcBczUMgejQ",
        "outputId": "1b9f248e-fcac-4cd1-ba71-37014a452476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instance: 0\n",
            "Coordinates: [[190.90116849279775, 590.4280175750611], [166.92708168528983, 530.2271396316246], [1085.991923849472, 16.922422544709608], [481.1400572117718, 1646.5622569153477], [13.17437595135608, 29.384015131643572], [579.5977925840228, 183.09074463391653], [3152.535909779576, 469.4891656526584], [56.79395127865876, 335.7676041800954], [140.16984710530528, 1415.9907047276663], [298.4056652503395, 432.6438567934434], [1329.4159078188572, 398.07979715224917], [184.34432775505695, 310.6223243278328], [1413.9458436791929, 71.70924057019577], [229.8380192520161, 12.507313429062522], [119.85732963663749, 267.10190426680424], [1068.128373699216, 72.16722331618915], [4.4735957948030896, 260.74107597220853], [315.9927811661698, 453.1032177042161], [74.37417532866615, 462.03510416808166], [391.5463212784245, 568.466736501489], [20.04917685531651, 200.4640408371322], [230.93189213865705, 1251.8991123066855], [1572.7308604790571, 269.1672960446028], [592.4233980658983, 398.928663581323], [270.91848807632886, 111.30947683362179], [937.6511284275634, 502.76960724392404], [379.12301291798457, 291.35789407895794], [423.7620301035877, 835.6244553900403], [531.8204189375267, 36.38573819950644], [17.693326797842897, 25.882043485803397]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for graph in graphs:\n",
        "    print(f\"Instance: {graph}\\nCoordinates: {graphs[graph]}\\n\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmflEwHUhD2b",
        "outputId": "fddc06aa-0bf2-4428-f56f-08d848b40a76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean coordinates of the 35th graph instance are:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[7207.304443622209, 7324.167128487795],\n",
              " [4163.142976744452, 433.4999525698357],\n",
              " [2490.932722533464, 4875.366088798042],\n",
              " [4551.5205625801955, 2982.122152631126],\n",
              " [6763.789886532631, 1030.9891543093809],\n",
              " [1192.5835434616927, 6609.37024934381],\n",
              " [4233.957584681043, 4034.951770206021],\n",
              " [2636.353709072561, 6386.539680951245],\n",
              " [5349.1546667631965, 9770.2454151319],\n",
              " [5350.938714803364, 7417.2756867157395],\n",
              " [3603.702035972396, 966.7842507669433],\n",
              " [8336.904519630016, 1307.823731512655],\n",
              " [2250.7038620979292, 6451.865150099256],\n",
              " [9410.524952419511, 6731.366335870505],\n",
              " [8976.457769795643, 8503.88983386449],\n",
              " [1713.7074001514418, 1096.4539677469254],\n",
              " [2822.1366695079887, 5066.67013828053],\n",
              " [3449.0565250305203, 9008.22163939093],\n",
              " [7599.90158431922, 6101.292179054742],\n",
              " [6282.968640320806, 5723.651901213904],\n",
              " [5377.336280017825, 658.9912628221895],\n",
              " [5782.466661739941, 3623.846327413507],\n",
              " [2612.0291245340845, 6706.035485758186],\n",
              " [7905.677924103369, 723.4527207413255],\n",
              " [8687.760132503685, 8977.531049384415],\n",
              " [3345.3485774394285, 1139.4106084946332],\n",
              " [3777.641694394105, 6568.454970385794],\n",
              " [8147.04280449171, 1032.2014753951414],\n",
              " [2170.319009545205, 1285.1867416777663],\n",
              " [2684.280522235982, 8614.963884506185]]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "print(f\"Euclidean coordinates of the 35th graph instance are:\")\n",
        "graphs[35]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7RZzLGThjCU",
        "outputId": "2cfc8f80-7a31-4794-f674-6d0f04f53280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 85 graphs each having 30 [x,y] coordinate points\n"
          ]
        }
      ],
      "source": [
        "print(f\"We have {len(graphs)} graphs each having {len(graphs[0])} [x,y] coordinate points\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-c6kYYrThu3n",
        "outputId": "4f8a3175-a6b7-49e4-b512-79c3b0e0f8ba"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-580e7637-ff2f-4525-b321-e8b932c9ae56\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-580e7637-ff2f-4525-b321-e8b932c9ae56\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving syn_tsp199_instance1.csv to syn_tsp199_instance1 (1).csv\n",
            "Saving syn_tsp198_instance1.csv to syn_tsp198_instance1 (1).csv\n",
            "Saving syn_tsp197_instance1.csv to syn_tsp197_instance1 (1).csv\n",
            "Saving syn_tsp196_instance1.csv to syn_tsp196_instance1 (1).csv\n",
            "Saving syn_tsp195_instance1.csv to syn_tsp195_instance1 (1).csv\n",
            "Saving syn_tsp194_instance1.csv to syn_tsp194_instance1 (1).csv\n",
            "Saving syn_tsp193_instance1.csv to syn_tsp193_instance1 (1).csv\n",
            "Saving syn_tsp192_instance1.csv to syn_tsp192_instance1 (1).csv\n",
            "Saving syn_tsp191_instance1.csv to syn_tsp191_instance1 (1).csv\n",
            "Saving syn_tsp190_instance1.csv to syn_tsp190_instance1 (1).csv\n",
            "Saving syn_tsp189_instance1.csv to syn_tsp189_instance1 (1).csv\n",
            "Saving syn_tsp188_instance1.csv to syn_tsp188_instance1 (1).csv\n",
            "Saving syn_tsp187_instance1.csv to syn_tsp187_instance1 (1).csv\n",
            "Saving syn_tsp186_instance1.csv to syn_tsp186_instance1 (1).csv\n",
            "Saving syn_tsp185_instance1.csv to syn_tsp185_instance1 (1).csv\n",
            "Saving syn_tsp139_instance1.csv to syn_tsp139_instance1 (1).csv\n",
            "Saving syn_tsp138_instance1.csv to syn_tsp138_instance1 (1).csv\n",
            "Saving syn_tsp137_instance1.csv to syn_tsp137_instance1 (1).csv\n",
            "Saving syn_tsp136_instance1.csv to syn_tsp136_instance1 (1).csv\n",
            "Saving syn_tsp135_instance1.csv to syn_tsp135_instance1 (1).csv\n",
            "Saving syn_tsp134_instance1.csv to syn_tsp134_instance1 (1).csv\n",
            "Saving syn_tsp133_instance1.csv to syn_tsp133_instance1 (1).csv\n",
            "Saving syn_tsp132_instance1.csv to syn_tsp132_instance1 (1).csv\n",
            "Saving syn_tsp131_instance1.csv to syn_tsp131_instance1 (1).csv\n",
            "Saving syn_tsp130_instance1.csv to syn_tsp130_instance1 (1).csv\n",
            "Saving syn_tsp129_instance1.csv to syn_tsp129_instance1 (1).csv\n",
            "Saving syn_tsp128_instance1.csv to syn_tsp128_instance1 (1).csv\n",
            "Saving syn_tsp127_instance1.csv to syn_tsp127_instance1 (1).csv\n",
            "Saving syn_tsp126_instance1.csv to syn_tsp126_instance1 (1).csv\n",
            "Saving syn_tsp125_instance1.csv to syn_tsp125_instance1 (1).csv\n",
            "Saving syn_tsp124_instance1.csv to syn_tsp124_instance1 (1).csv\n",
            "Saving syn_tsp123_instance1.csv to syn_tsp123_instance1 (1).csv\n",
            "Saving syn_tsp122_instance1.csv to syn_tsp122_instance1 (1).csv\n",
            "Saving syn_tsp121_instance1.csv to syn_tsp121_instance1 (1).csv\n",
            "Saving syn_tsp120_instance1.csv to syn_tsp120_instance1 (1).csv\n",
            "Saving syn_tsp119_instance1.csv to syn_tsp119_instance1 (1).csv\n",
            "Saving syn_tsp118_instance1.csv to syn_tsp118_instance1 (1).csv\n",
            "Saving syn_tsp117_instance1.csv to syn_tsp117_instance1 (1).csv\n",
            "Saving syn_tsp116_instance1.csv to syn_tsp116_instance1 (1).csv\n",
            "Saving syn_tsp115_instance1.csv to syn_tsp115_instance1 (1).csv\n",
            "Saving syn_tsp114_instance1.csv to syn_tsp114_instance1 (1).csv\n",
            "Saving syn_tsp113_instance1.csv to syn_tsp113_instance1 (1).csv\n",
            "Saving syn_tsp112_instance1.csv to syn_tsp112_instance1 (1).csv\n",
            "Saving syn_tsp111_instance1.csv to syn_tsp111_instance1 (1).csv\n",
            "Saving syn_tsp110_instance1.csv to syn_tsp110_instance1 (1).csv\n",
            "Saving syn_tsp109_instance1.csv to syn_tsp109_instance1 (1).csv\n",
            "Saving syn_tsp108_instance1.csv to syn_tsp108_instance1 (1).csv\n",
            "Saving syn_tsp107_instance1.csv to syn_tsp107_instance1 (1).csv\n",
            "Saving syn_tsp106_instance1.csv to syn_tsp106_instance1 (1).csv\n",
            "Saving syn_tsp105_instance1.csv to syn_tsp105_instance1 (1).csv\n",
            "Saving syn_tsp104_instance1.csv to syn_tsp104_instance1 (1).csv\n",
            "Saving syn_tsp103_instance1.csv to syn_tsp103_instance1 (1).csv\n",
            "Saving syn_tsp102_instance1.csv to syn_tsp102_instance1 (1).csv\n",
            "Saving syn_tsp101_instance1.csv to syn_tsp101_instance1 (1).csv\n",
            "Saving syn_tsp99_instance1.csv to syn_tsp99_instance1 (1).csv\n",
            "Saving syn_tsp98_instance1.csv to syn_tsp98_instance1 (1).csv\n",
            "Saving syn_tsp97_instance1.csv to syn_tsp97_instance1 (1).csv\n",
            "Saving syn_tsp96_instance1.csv to syn_tsp96_instance1 (1).csv\n",
            "Saving syn_tsp95_instance1.csv to syn_tsp95_instance1 (1).csv\n",
            "Saving syn_tsp94_instance1.csv to syn_tsp94_instance1 (1).csv\n",
            "Saving syn_tsp93_instance1.csv to syn_tsp93_instance1 (1).csv\n",
            "Saving syn_tsp92_instance1.csv to syn_tsp92_instance1 (1).csv\n",
            "Saving syn_tsp91_instance1.csv to syn_tsp91_instance1 (1).csv\n",
            "Saving syn_tsp90_instance1.csv to syn_tsp90_instance1 (1).csv\n",
            "Saving syn_tsp89_instance1.csv to syn_tsp89_instance1 (1).csv\n",
            "Saving syn_tsp88_instance1.csv to syn_tsp88_instance1 (1).csv\n",
            "Saving syn_tsp87_instance1.csv to syn_tsp87_instance1 (1).csv\n",
            "Saving syn_tsp86_instance1.csv to syn_tsp86_instance1 (1).csv\n",
            "Saving syn_tsp85_instance1.csv to syn_tsp85_instance1 (1).csv\n",
            "Saving syn_tsp84_instance1.csv to syn_tsp84_instance1 (1).csv\n",
            "Saving syn_tsp83_instance1.csv to syn_tsp83_instance1 (1).csv\n",
            "Saving syn_tsp82_instance1.csv to syn_tsp82_instance1 (1).csv\n",
            "Saving syn_tsp81_instance1.csv to syn_tsp81_instance1 (1).csv\n",
            "Saving syn_tsp79_instance1.csv to syn_tsp79_instance1 (1).csv\n",
            "Saving syn_tsp78_instance1.csv to syn_tsp78_instance1 (1).csv\n",
            "Saving syn_tsp77_instance1.csv to syn_tsp77_instance1 (1).csv\n",
            "Saving syn_tsp76_instance1.csv to syn_tsp76_instance1 (1).csv\n",
            "Saving syn_tsp75_instance1.csv to syn_tsp75_instance1 (1).csv\n",
            "Saving syn_tsp74_instance1.csv to syn_tsp74_instance1 (1).csv\n",
            "Saving syn_tsp73_instance1.csv to syn_tsp73_instance1 (1).csv\n",
            "Saving syn_tsp72_instance1.csv to syn_tsp72_instance1 (1).csv\n",
            "Saving syn_tsp71_instance1.csv to syn_tsp71_instance1 (1).csv\n",
            "Saving syn_tsp70_instance1.csv to syn_tsp70_instance1 (1).csv\n",
            "Saving syn_tsp69_instance1.csv to syn_tsp69_instance1 (1).csv\n",
            "Saving syn_tsp68_instance1.csv to syn_tsp68_instance1 (1).csv\n"
          ]
        }
      ],
      "source": [
        "# upload training data\n",
        "from google.colab import files\n",
        "\n",
        "uploaded_2 = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "0r3x8O_DiOAd"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import io\n",
        "\n",
        "runs = {}\n",
        "index = 0\n",
        "\n",
        "for filename, file_data in uploaded_2.items():\n",
        "    file_content = file_data.decode('utf-8')\n",
        "\n",
        "    instance_data = []\n",
        "    csv_reader = csv.reader(io.StringIO(file_content))\n",
        "\n",
        "    next(csv_reader, None)\n",
        "\n",
        "    for row in csv_reader:\n",
        "        relaxation_param = float(row[0])\n",
        "        p_f = float(row[1])\n",
        "        e_std = float(row[2])\n",
        "        e_avg = float(row[3])\n",
        "        e_min = float(row[4])\n",
        "\n",
        "        instance_data.append([relaxation_param, p_f, e_std, e_avg, e_min])\n",
        "\n",
        "    runs[index] = instance_data\n",
        "    index+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRbr1MM9k3qT",
        "outputId": "0d836614-5f7d-4d6e-89cd-0aec54c43aec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now we have all the information. \n",
            "The runs list contains information about 85 graphs. \n",
            "Each of which is tested for 100 different relaxation parameters. \n",
            "For each such parameter we have a vector of length 5 information containing the values of the energies as [A, p_f, e_std, e_avg, e_min] extracted from the annealing experiment\n"
          ]
        }
      ],
      "source": [
        "print(f\"Now we have all the information. \\nThe runs list contains information about {len(runs)} graphs. \\nEach of which is tested for {len(runs[0])} different relaxation parameters. \\nFor each such parameter we have a vector of length {len(runs[0][0])} information containing the values of the energies as [A, p_f, e_std, e_avg, e_min] extracted from the annealing experiment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PTvwozvlbiL"
      },
      "source": [
        "`data[i][j] = [relaxation_parameter, p_f, e_std, e_avg, e_min]` for the $j^{th}$ run of the $i^{th}$ instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yrBKB2Sl2V2",
        "outputId": "f5db2cb1-e0c6-41bc-f9cd-a46188e68f0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1952.0, 0.046875, 19742.68658959116, 1589.5882639783756, 16002.903095621281]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "runs[0][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBbHToDtmP20"
      },
      "source": [
        "#### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "VC1qd4wZMXMf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "normed_graphs = {}\n",
        "\n",
        "for index, coordinates in graphs.items():\n",
        "    if isinstance(coordinates, list) and all(isinstance(coord, list) and len(coord) == 2 for coord in coordinates):\n",
        "        x_coords = [coord[0] for coord in coordinates]\n",
        "        y_coords = [coord[1] for coord in coordinates]\n",
        "\n",
        "        mean_x = np.mean(x_coords)\n",
        "        stddev_x = np.std(x_coords)\n",
        "        mean_y = np.mean(y_coords)\n",
        "        stddev_y = np.std(y_coords)\n",
        "\n",
        "        # Normalize formula\n",
        "        normalized_coords = [\n",
        "            [(x - mean_x) / stddev_x, (y - mean_y) / stddev_y] for x, y in coordinates\n",
        "        ]\n",
        "\n",
        "        normed_graphs[index] = normalized_coords\n",
        "    else:\n",
        "        print(f\"Skipping instance {filename}: Invalid data format\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CUy0NtFJpIR",
        "outputId": "779ba823-78d5-4e75-ec3a-a38ab9b887d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(76,\n",
              " [[1.1880318080540273, 1.103539991690836],\n",
              "  [0.6784669030525027, 0.5254224167293567],\n",
              "  [-0.3775775719241746, 0.8981488533167771],\n",
              "  [0.8348328047005558, -1.9438196414559994],\n",
              "  [-0.08084364922665153, -0.49351595558707856],\n",
              "  [0.3688659438725222, 0.5481962830734642],\n",
              "  [-0.3634870344833535, 0.08141804823360979],\n",
              "  [0.6250094963256145, -1.3272812130063205],\n",
              "  [0.22432309169430525, -2.05645549302264],\n",
              "  [-0.238591089518351, 1.2000368145186942],\n",
              "  [0.4814180117975955, 1.072191681123153],\n",
              "  [-1.3975391287447516, -0.4410599364730879],\n",
              "  [1.4409917127988712, 0.881639220674776],\n",
              "  [1.416721379338304, -0.0126995844288734],\n",
              "  [-1.5793293443571046, -1.0581705755512094],\n",
              "  [0.04926159519684329, -0.17661497025644887],\n",
              "  [0.7486429391828898, 0.23689668495894267],\n",
              "  [-1.3262506711218605, 1.0846339462864933],\n",
              "  [-0.477637890303696, 0.22306055785609838],\n",
              "  [-0.0050413334163999885, -1.4313510225843378],\n",
              "  [-1.634972690666217, -1.211574346406075],\n",
              "  [0.5167390303239235, 0.06282856760914733],\n",
              "  [1.5710694817457937, 0.9032808828235501],\n",
              "  [-1.635266624730115, -1.9293826515187975],\n",
              "  [0.7999006045489646, -0.030516134591722675],\n",
              "  [-1.4630376686693984, 1.09270562653641],\n",
              "  [1.457161925057766, 1.214839204037927],\n",
              "  [-1.4678331859779876, 0.6932775232775527],\n",
              "  [-0.1706958112311401, 0.40219811643727443],\n",
              "  [-0.1833330333192833, -0.11187289430146297]])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "ith_run= np.random.randint(100)\n",
        "ith_run,normed_graphs[ith_run]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "iyM_XpnWOg1Y",
        "outputId": "94e6ce62-1b8a-470f-f413-3d169a777ee9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x = [coord[0] for coord in normed_graphs[ith_run]]\n",
        "y = [coord[1] for coord in normed_graphs[ith_run]]\n",
        "plt.scatter(x,y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "5BzCJHsNrlwp"
      },
      "outputs": [],
      "source": [
        "for index in normed_graphs:\n",
        "  normed_graphs[index] = np.array(normed_graphs[index]).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydBwbP_PsWys",
        "outputId": "46baa4f9-15ec-44fc-a2af-297dc248c823"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60,\n",
              " array([-0.53957575,  0.4398966 , -0.57636841,  0.28891542,  0.83410605,\n",
              "        -0.99843048, -0.09415066,  3.08863531, -0.81233028, -0.96717734,\n",
              "         0.05695089, -0.58168756,  4.00559883,  0.13658722, -0.745388  ,\n",
              "        -0.19878064, -0.61743232,  2.51037189, -0.37459028,  0.04418079,\n",
              "         1.20768504, -0.04250437, -0.54963843, -0.2618439 ,  1.33741181,\n",
              "        -0.86102752, -0.47981997, -1.00950338, -0.64860562, -0.37099122,\n",
              "         0.80669114, -0.85987892, -0.82568323, -0.3869439 , -0.3475996 ,\n",
              "         0.09549198, -0.7184079 ,  0.11789276, -0.23164877,  0.38481866,\n",
              "        -0.80177963, -0.53811608, -0.47814122,  2.09883731,  1.5810967 ,\n",
              "        -0.36581131,  0.07663415, -0.04037545, -0.41677442, -0.76171185,\n",
              "         0.60644975,  0.22005312, -0.25071462, -0.31015826, -0.18220782,\n",
              "         1.05483859, -0.0163723 , -0.94961733, -0.80539512, -0.97596014]))"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "len(normed_graphs[0]), normed_graphs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "cb5lxfJDM_rN"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMxuZGBLNENe"
      },
      "source": [
        "Now the instance coordinates are all normalized as per the formula $x^j_i (norm) = (x^j_i - \\mu^j)/ \\sigma^j$ where $i$ denotes the $i^{th}$ coordinate and $j$ denotes the $j^{th}$ graph instance indexed from 0 -> 84.\n",
        "\n",
        "#### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chbiqwoMPlCh",
        "outputId": "e3e727c8-6144-4d62-8a0b-d839ca3ee8cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 60])\n"
          ]
        }
      ],
      "source": [
        "instance_dict = {'train': [], 'val': []}\n",
        "dataset_sizes = {'train': 0, 'val': 0}\n",
        "\n",
        "for i in range(len(normed_graphs)):\n",
        "  if np.random.rand() < 0.8:\n",
        "    instance_dict['train'].append(normed_graphs[i])\n",
        "    dataset_sizes['train'] += 1\n",
        "  else:\n",
        "    instance_dict['val'].append(normed_graphs[i])\n",
        "    dataset_sizes['val'] += 1\n",
        "\n",
        "class TSPDataset(Dataset):\n",
        "    def __init__(self, instances):\n",
        "        self.instances = instances\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.instances)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        coordinates = torch.tensor(self.instances[idx], dtype=torch.float32)\n",
        "        return coordinates\n",
        "\n",
        "datasets = {x: TSPDataset(instance_dict[x]) for x in ['train', 'val']}\n",
        "dataloaders = {x: DataLoader(datasets[x], batch_size=16, shuffle=True) for x in ['train', 'val']}\n",
        "\n",
        "for instance_data in dataloaders['train']:\n",
        "    print(instance_data.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EKYMdj83gk3"
      },
      "source": [
        "### The Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "0jzkmEJI3ltf"
      },
      "outputs": [],
      "source": [
        "class Autoencoder_TSP(nn.Module):\n",
        "    def __init__(self, bottleneck):\n",
        "        super(Autoencoder_TSP, self).__init__()\n",
        "\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "        self.enc1 = nn.Linear(60, 45)\n",
        "        self.enc2 = nn.Linear(45, bottleneck)\n",
        "        self.dec1 = nn.Linear(bottleneck, 45)\n",
        "        self.dec2 = nn.Linear(45, 60)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.enc1(x)\n",
        "        x = self.sig(x)\n",
        "        x = self.enc2(x)\n",
        "        x = self.sig(x)\n",
        "        x = self.dec1(x)\n",
        "        x = self.sig(x)\n",
        "        x = self.dec2(x)\n",
        "        x = self.sig(x)\n",
        "        x = 2*x - 1             # to re-adjust x value to the interval [-1,1]\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsnxKipq8mnT"
      },
      "source": [
        "### The Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "cHatSVQWAXug"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "fWn13iZw8ilR"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, scheduler, lambda1 = 0, num_epochs = 25):\n",
        "\n",
        "  since = time.time()\n",
        "\n",
        "  best_model_wts = deepcopy(model.state_dict())\n",
        "  best_loss = 1000\n",
        "\n",
        "  _loss = {'train': [], 'val': []}\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    if (epoch+1) % 10 == 0 or epoch == 0:\n",
        "      print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "      print('-' * 10)\n",
        "\n",
        "    # each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "      if phase == 'train':\n",
        "        model.train()     # set model to training mode\n",
        "      else:\n",
        "        model.eval()      # set model to validation mode\n",
        "\n",
        "      running_loss = 0\n",
        "\n",
        "      # iterate over data\n",
        "      for inputs in dataloaders[phase]:\n",
        "        inputs = inputs.to(device)\n",
        "        # .to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          output = model(inputs)\n",
        "          loss = criterion(output, inputs)\n",
        "\n",
        "          # Regularization if necessary\n",
        "\n",
        "          # backward + optimize only if training\n",
        "          if phase == 'train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "      if phase == 'train':\n",
        "        scheduler.step()\n",
        "\n",
        "      epoch_loss = running_loss/dataset_sizes[phase]\n",
        "\n",
        "      if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "        print(f'{phase} Loss: {epoch_loss:.4f}')\n",
        "\n",
        "      _loss[phase].append(epoch_loss)\n",
        "\n",
        "      if phase == 'val' and epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        best_model_wts = deepcopy(model.state_dict())\n",
        "\n",
        "  time_elapsed = time.time() - since\n",
        "  print(f'Training completed in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "  print(f'Best val loss: {best_loss:.4f}')\n",
        "\n",
        "  # Load best model weights and return\n",
        "  model.load_state_dict(best_model_wts)\n",
        "  return model, _loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "CBDXjoB9eb3G"
      },
      "outputs": [],
      "source": [
        "# using GPU/CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U7hkt4scQks",
        "outputId": "8334b748-fc64-4a03-dcda-7a85223f5e4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with lambda2 = 1e-06\n",
            "\n",
            "Epoch 1/100\n",
            "----------\n",
            "train Loss: 0.0777\n",
            "val Loss: 0.1053\n",
            "\n",
            "Epoch 10/100\n",
            "----------\n",
            "train Loss: 0.0754\n",
            "val Loss: 0.1072\n",
            "\n",
            "Epoch 20/100\n",
            "----------\n",
            "train Loss: 0.0748\n",
            "val Loss: 0.1071\n",
            "\n",
            "Epoch 30/100\n",
            "----------\n",
            "train Loss: 0.0750\n",
            "val Loss: 0.1084\n",
            "\n",
            "Epoch 40/100\n",
            "----------\n",
            "train Loss: 0.0740\n",
            "val Loss: 0.1077\n",
            "\n",
            "Epoch 50/100\n",
            "----------\n",
            "train Loss: 0.0747\n",
            "val Loss: 0.1076\n",
            "\n",
            "Epoch 60/100\n",
            "----------\n",
            "train Loss: 0.0742\n",
            "val Loss: 0.1077\n",
            "\n",
            "Epoch 70/100\n",
            "----------\n",
            "train Loss: 0.0742\n",
            "val Loss: 0.1077\n",
            "\n",
            "Epoch 80/100\n",
            "----------\n",
            "train Loss: 0.0741\n",
            "val Loss: 0.1081\n",
            "\n",
            "Epoch 90/100\n",
            "----------\n",
            "train Loss: 0.0742\n",
            "val Loss: 0.1071\n",
            "\n",
            "Epoch 100/100\n",
            "----------\n",
            "train Loss: 0.0740\n",
            "val Loss: 0.1057\n",
            "Training completed in 0m 1s\n",
            "Best val loss: 0.1053\n",
            "====================\n"
          ]
        }
      ],
      "source": [
        "lambda1 = [1e-6]#, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
        "lr = [0.5]#, 0.003, 0.005, 0.005, 0.008, 0.01]\n",
        "Loss = {'train': [], 'val': []}\n",
        "bottleneck = 30\n",
        "\n",
        "for i in range(len(lambda1)):\n",
        "\n",
        "  print(f\"Training with lambda2 = {lambda1[i]}\")\n",
        "  net = Autoencoder_TSP(bottleneck)\n",
        "  net.to(device)\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = optim.SGD(net.parameters(), lr = lr[i], momentum = 0.9)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 10, gamma = 0.5)\n",
        "  num_epochs = 100\n",
        "\n",
        "  net, _loss = train(net, criterion, optimizer, scheduler, lambda1[i], num_epochs)\n",
        "\n",
        "  for phase in ['train', 'val']:\n",
        "    Loss[phase].append(_loss[phase])\n",
        "\n",
        "  print('='*20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "N3B8CUqstyVE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}